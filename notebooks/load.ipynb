{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd9bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sqlalchemy\n",
    "#! pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acb3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1d5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"postgres\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 5432\n",
    "DB = \"sales_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efae41d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7ad853f12840>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(f'postgresql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}')\n",
    "# Test Connection\n",
    "engine .connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ca492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 12:09:29 WARN Utils: Your hostname, codespaces-240bfe resolves to a loopback address: 127.0.0.1; using 10.0.11.180 instead (on interface eth0)\n",
      "25/04/14 12:09:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 12:09:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.driver.memory\", \"12g\") \\\n",
    "        .appName(\"ParquetToPostgres\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8507b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_iterator(iterator, chunk_size):\n",
    "    \"\"\"\n",
    "    Yield successive chunks of a given size from an iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Use itertools.islice to grab chunk_size elements from the iterator.\n",
    "        chunk = list(itertools.islice(iterator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb66f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_to_postgres_in_chunks(spark_df, table_name, engine=engine, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Write data from a Spark DataFrame to a PostgreSQL table in chunks.\n",
    "    \"\"\"\n",
    "    total_rows = 0\n",
    "    try:\n",
    "        # Create an iterator over the DataFrame's rows.\n",
    "        iterator = spark_df.toLocalIterator()\n",
    "        for chunk_index, chunk in enumerate(chunk_iterator(iterator, chunk_size), start=1):\n",
    "            # Convert each Row object to a dictionary, then to a Pandas DataFrame.\n",
    "            chunk_dicts = [row.asDict() for row in chunk]\n",
    "            pdf = pd.DataFrame(chunk_dicts)\n",
    "            pdf.to_sql(name=table_name, con=engine, if_exists=\"append\", index=False)\n",
    "            total_rows += len(pdf)\n",
    "            print(f\"Inserted chunk {chunk_index} with {len(pdf)} rows; Total inserted rows: {total_rows}\")\n",
    "        print(f\"Successfully wrote data to PostgreSQL table '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b56163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_postgres(df_spark, table_name):    \n",
    "    try:\n",
    "        # Convert Spark DF to Pandas before writing via SQLAlchemy\n",
    "        df_pandas = df_spark.toPandas()\n",
    "\n",
    "        # Load to Postgres; replace table if it exists (or use 'append', etc.)\n",
    "        df_pandas.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"Successfully wrote data to PostgreSQL table '{table_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2965efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"categories\",\n",
    "    \"cities\",\n",
    "    \"countries\",\n",
    "    \"customers\",\n",
    "    \"employees\",\n",
    "    \"products\",\n",
    "    \"sales\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562504ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading categories data into PostgreSQL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../../data/datalake/categories/*.parquet with 11 records.\n",
      "Successfully wrote data to PostgreSQL table 'categories'.\n",
      "Loading cities data into PostgreSQL.\n",
      "Loading data from ../../data/datalake/cities/*.parquet with 96 records.\n",
      "Successfully wrote data to PostgreSQL table 'cities'.\n",
      "Loading countries data into PostgreSQL.\n",
      "Loading data from ../../data/datalake/countries/*.parquet with 206 records.\n",
      "Successfully wrote data to PostgreSQL table 'countries'.\n",
      "Loading customers data into PostgreSQL.\n",
      "Loading data from ../../data/datalake/customers/*.parquet with 98759 records.\n",
      "Successfully wrote data to PostgreSQL table 'customers'.\n",
      "Loading employees data into PostgreSQL.\n",
      "Loading data from ../../data/datalake/employees/*.parquet with 23 records.\n",
      "Successfully wrote data to PostgreSQL table 'employees'.\n",
      "Loading products data into PostgreSQL.\n",
      "Loading data from ../../data/datalake/products/*.parquet with 452 records.\n",
      "Successfully wrote data to PostgreSQL table 'products'.\n",
      "Loading sales data into PostgreSQL.\n",
      "Loading data from ../../data/datalake/sales/*.parquet with 6758125 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 12:09:44 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunk 1 with 100000 rows; Total inserted rows: 100000\n",
      "Inserted chunk 2 with 100000 rows; Total inserted rows: 200000\n",
      "Inserted chunk 3 with 100000 rows; Total inserted rows: 300000\n",
      "Inserted chunk 4 with 100000 rows; Total inserted rows: 400000\n",
      "Inserted chunk 5 with 100000 rows; Total inserted rows: 500000\n",
      "Inserted chunk 6 with 100000 rows; Total inserted rows: 600000\n",
      "Inserted chunk 7 with 100000 rows; Total inserted rows: 700000\n",
      "Inserted chunk 8 with 100000 rows; Total inserted rows: 800000\n",
      "Inserted chunk 9 with 100000 rows; Total inserted rows: 900000\n",
      "Inserted chunk 10 with 100000 rows; Total inserted rows: 1000000\n",
      "Inserted chunk 11 with 100000 rows; Total inserted rows: 1100000\n",
      "Inserted chunk 12 with 100000 rows; Total inserted rows: 1200000\n",
      "Inserted chunk 13 with 100000 rows; Total inserted rows: 1300000\n",
      "Inserted chunk 14 with 100000 rows; Total inserted rows: 1400000\n",
      "Inserted chunk 15 with 100000 rows; Total inserted rows: 1500000\n",
      "Inserted chunk 16 with 100000 rows; Total inserted rows: 1600000\n",
      "Inserted chunk 17 with 100000 rows; Total inserted rows: 1700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunk 18 with 100000 rows; Total inserted rows: 1800000\n",
      "Inserted chunk 19 with 100000 rows; Total inserted rows: 1900000\n",
      "Inserted chunk 20 with 100000 rows; Total inserted rows: 2000000\n",
      "Inserted chunk 21 with 100000 rows; Total inserted rows: 2100000\n",
      "Inserted chunk 22 with 100000 rows; Total inserted rows: 2200000\n",
      "Inserted chunk 23 with 100000 rows; Total inserted rows: 2300000\n",
      "Inserted chunk 24 with 100000 rows; Total inserted rows: 2400000\n",
      "Inserted chunk 25 with 100000 rows; Total inserted rows: 2500000\n",
      "Inserted chunk 26 with 100000 rows; Total inserted rows: 2600000\n",
      "Inserted chunk 27 with 100000 rows; Total inserted rows: 2700000\n",
      "Inserted chunk 28 with 100000 rows; Total inserted rows: 2800000\n",
      "Inserted chunk 29 with 100000 rows; Total inserted rows: 2900000\n",
      "Inserted chunk 30 with 100000 rows; Total inserted rows: 3000000\n",
      "Inserted chunk 31 with 100000 rows; Total inserted rows: 3100000\n",
      "Inserted chunk 32 with 100000 rows; Total inserted rows: 3200000\n",
      "Inserted chunk 33 with 100000 rows; Total inserted rows: 3300000\n",
      "Inserted chunk 34 with 100000 rows; Total inserted rows: 3400000\n",
      "Inserted chunk 35 with 100000 rows; Total inserted rows: 3500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunk 36 with 100000 rows; Total inserted rows: 3600000\n",
      "Inserted chunk 37 with 100000 rows; Total inserted rows: 3700000\n",
      "Inserted chunk 38 with 100000 rows; Total inserted rows: 3800000\n",
      "Inserted chunk 39 with 100000 rows; Total inserted rows: 3900000\n",
      "Inserted chunk 40 with 100000 rows; Total inserted rows: 4000000\n",
      "Inserted chunk 41 with 100000 rows; Total inserted rows: 4100000\n",
      "Inserted chunk 42 with 100000 rows; Total inserted rows: 4200000\n",
      "Inserted chunk 43 with 100000 rows; Total inserted rows: 4300000\n",
      "Inserted chunk 44 with 100000 rows; Total inserted rows: 4400000\n",
      "Inserted chunk 45 with 100000 rows; Total inserted rows: 4500000\n",
      "Inserted chunk 46 with 100000 rows; Total inserted rows: 4600000\n",
      "Inserted chunk 47 with 100000 rows; Total inserted rows: 4700000\n",
      "Inserted chunk 48 with 100000 rows; Total inserted rows: 4800000\n",
      "Inserted chunk 49 with 100000 rows; Total inserted rows: 4900000\n",
      "Inserted chunk 50 with 100000 rows; Total inserted rows: 5000000\n",
      "Inserted chunk 51 with 100000 rows; Total inserted rows: 5100000\n",
      "Inserted chunk 52 with 100000 rows; Total inserted rows: 5200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunk 53 with 100000 rows; Total inserted rows: 5300000\n",
      "Inserted chunk 54 with 100000 rows; Total inserted rows: 5400000\n",
      "Inserted chunk 55 with 100000 rows; Total inserted rows: 5500000\n",
      "Inserted chunk 56 with 100000 rows; Total inserted rows: 5600000\n",
      "Inserted chunk 57 with 100000 rows; Total inserted rows: 5700000\n",
      "Inserted chunk 58 with 100000 rows; Total inserted rows: 5800000\n",
      "Inserted chunk 59 with 100000 rows; Total inserted rows: 5900000\n",
      "Inserted chunk 60 with 100000 rows; Total inserted rows: 6000000\n",
      "Inserted chunk 61 with 100000 rows; Total inserted rows: 6100000\n",
      "Inserted chunk 62 with 100000 rows; Total inserted rows: 6200000\n",
      "Inserted chunk 63 with 100000 rows; Total inserted rows: 6300000\n",
      "Inserted chunk 64 with 100000 rows; Total inserted rows: 6400000\n",
      "Inserted chunk 65 with 100000 rows; Total inserted rows: 6500000\n",
      "Inserted chunk 66 with 100000 rows; Total inserted rows: 6600000\n",
      "Inserted chunk 67 with 100000 rows; Total inserted rows: 6700000\n",
      "Inserted chunk 68 with 58125 rows; Total inserted rows: 6758125\n"
     ]
    }
   ],
   "source": [
    "for table in tables:\n",
    "    print(f\"Loading {table} data into PostgreSQL.\")\n",
    "    parquet_path = f\"../data/datalake/{table}/*.parquet\"\n",
    "    df_spark = spark.read.parquet(parquet_path)\n",
    "    print(f\"Loading data from {parquet_path} with {df_spark.count()} records.\")\n",
    "    if df_spark.count()> 100000:\n",
    "        load_to_postgres_in_chunks(df_spark, table)\n",
    "    else:\n",
    "        load_to_postgres(df_spark, table)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
